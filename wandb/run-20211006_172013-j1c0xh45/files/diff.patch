diff --git a/rlhpd/DQfD_training.py b/rlhpd/DQfD_training.py
index b63f8f4..ad32db6 100644
--- a/rlhpd/DQfD_training.py
+++ b/rlhpd/DQfD_training.py
@@ -43,9 +43,6 @@ def train(
     epsilon
 ):
     
-    # init tensorboard writer
-    writer = SummaryWriter(log_dir)
-    
     # init optimizer
     optimizer = torch.optim.AdamW(q_net.parameters(), lr=lr, weight_decay=weight_decay)
     
@@ -125,7 +122,7 @@ def train(
 
             ## compute loss
             # J_DQ
-            J_DQ = (reward + discount_factor * next_q_values[np.arange(len(next_q_values), next_target_q_action] - q_values[np.arange(len(q_values)), action]) ** 2
+            J_DQ = (reward + discount_factor * next_q_values[np.arange(len(next_q_values)), next_target_q_action] - q_values[np.arange(len(q_values)), action]) ** 2
             
             # J_E
             pre_max_q = q_values + supervised_loss_margin
@@ -145,9 +142,9 @@ def train(
             # loss logging
             log_dict = {
                 'Training/total_loss': loss,
-                'Training/J_DQ': J_DQ.mean()
+                'Training/J_DQ': J_DQ.mean(),
                 'Training/J_E': J_E.sum() / expert_mask.sum(),
-                'Training/J_n', J_n.mean(),
+                'Training/J_n': J_n.mean(),
                 'Training/Step': steps
             }
             wandb.log(log_dict)
@@ -241,7 +238,6 @@ if __name__ == '__main__':
     parser.add_argument('--batch_size', type=int, default=100)
     parser.add_argument('--save_freq', type=int, default=100)
     parser.add_argument('--update_freq', type=int, default=100)
-    parser.add_argument('--action_repeat', type=int, default=1)
     parser.add_argument('--lr', type=float, default=3e-4)
     parser.add_argument('--epsilon', type=float, default=0.01)
     parser.add_argument('--PER_exponent', type=float, default=0.4, help='PER exponent')
diff --git a/rlhpd/common/DQfD_utils.py b/rlhpd/common/DQfD_utils.py
index f704d09..0b4af5d 100644
--- a/rlhpd/common/DQfD_utils.py
+++ b/rlhpd/common/DQfD_utils.py
@@ -9,7 +9,7 @@ from torch.utils.data import Dataset
 import minerl
 import gym
 
-from action_shaping import find_cave_action, make_waterfall_action, build_house_action, create_pen_action
+from common.action_shaping import find_cave_action, make_waterfall_action, build_house_action, create_pen_action
 
 Transition = namedtuple('Transition',
                         ('state', 'action', 'next_state', 'reward', 'n_step_state', 'n_step_reward', 'td_error'))
