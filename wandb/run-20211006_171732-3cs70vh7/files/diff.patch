diff --git a/rlhpd/DQfD_training.py b/rlhpd/DQfD_training.py
index b63f8f4..39a243d 100644
--- a/rlhpd/DQfD_training.py
+++ b/rlhpd/DQfD_training.py
@@ -43,9 +43,6 @@ def train(
     epsilon
 ):
     
-    # init tensorboard writer
-    writer = SummaryWriter(log_dir)
-    
     # init optimizer
     optimizer = torch.optim.AdamW(q_net.parameters(), lr=lr, weight_decay=weight_decay)
     
diff --git a/rlhpd/common/DQfD_utils.py b/rlhpd/common/DQfD_utils.py
index f704d09..0b4af5d 100644
--- a/rlhpd/common/DQfD_utils.py
+++ b/rlhpd/common/DQfD_utils.py
@@ -9,7 +9,7 @@ from torch.utils.data import Dataset
 import minerl
 import gym
 
-from action_shaping import find_cave_action, make_waterfall_action, build_house_action, create_pen_action
+from common.action_shaping import find_cave_action, make_waterfall_action, build_house_action, create_pen_action
 
 Transition = namedtuple('Transition',
                         ('state', 'action', 'next_state', 'reward', 'n_step_state', 'n_step_reward', 'td_error'))
