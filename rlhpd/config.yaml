# This is a central config file to store all configuration parameters for RLHPD.

# Example usage:
# from common import utils
# cfg = utils.load_config("config.yaml")
# # Returns a Munch file which is a dictionary-like object that supports
# # attribute-style access (object.attribute.path)
# # See: https://github.com/Infinidat/munch
# print(cfg.out.base_dir)

env_task: MineRLBasaltFindCave-v0
demos_dir: /home/junshern.chan/git/basalt-notyourrl/data
wandb_entity: junshern

out:
    base_dir: ./output

pretrain_policy:
    model_path: ./output/Q_0.pth

sampler:
    db_path: ./output/annotation_buffer.db
    traj_dir: ./output/trajectories/
    rnd_seed: 0
    max_traj_length: 100
    num_traj: 5
    num_samples: 5
    sample_length: 20
    pair_per_sample: 2
    autolabel: True
    autolabel_per_sample: 1

rate_ui:
    videos_dir: ./output/videos
    video_fps: 20

reward:
    wandb_project: 4-train-reward
    save_dir: ./output/reward_models
    best_model_path: ./output/reward_models/best.pt
    # Training params
    rand_seed: 1
    max_num_pairs: null
    val_split: 0.1
    val_every_n_batch: 100
    save_every_n_epoch: 1
    num_epochs: 5
    gaussian_prior_weight: 0.1
    # The model is trained on batches of 16 segment pairs (see below), 
    # optimized with Adam (Kingma and Ba, 2014) 
    # with learning rate 0.0003, β1 = 0.9, β2 = 0.999, and eps = 10^−8 .
    batch_size: 16
    adam_lr: 0.0003
    adam_betas: [0.9, 0.999]
    adam_eps: 0.00000001 # 1e-8
