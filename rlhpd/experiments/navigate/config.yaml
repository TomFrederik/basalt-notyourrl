# This is a central config file to store all configuration parameters for RLHPD.

# Example usage:
# from common import utils
# cfg = utils.load_config("config.yaml")
# # Returns a Munch file which is a dictionary-like object that supports
# # attribute-style access (object.attribute.path)
# # See: https://github.com/Infinidat/munch
# print(cfg.my.param.value)

env_task: MineRLNavigateDense-v0
demos_dir: /home/junshern.chan/git/basalt-notyourrl/data
wandb_entity: 'junshern'

clips:
    dir: ./output/clips
    clip_length: 100 # 20(?) frames per second -> 5 seconds
    num_clips: 100

pref_ui:
    videos_dir: ./output/videos

reward:
    wandb_project: DRLfHP-navigate
    save_dir: ./output/reward_models
    best_model_path: ./output/reward_models/best.pt
    # Training params
    rand_seed: 1
    max_num_pairs: None
    val_split: 0.1
    val_every_n_batch: 100
    save_every_n_epoch: 1
    num_epochs: 10
    gaussian_prior_weight: 0.1
    # The model is trained on batches of 16 segment pairs (see below), 
    # optimized with Adam (Kingma and Ba, 2014) 
    # with learning rate 0.0003, β1 = 0.9, β2 = 0.999, and eps = 10^−8 .
    batch_size: 16
    adam_lr: 0.0003
    adam_betas: [0.9, 0.999]
    adam_eps: 0.00000001 # 1e-8

policy:
    wandb_project: policy-navigate
    # Training params
    rand_seed: 0
    train_steps: 10000
    gym_env: MineRLNavigateDenseVectorObf-v0
    reward_model_path: ./output/reward_models/best.pt
    policy_models_dir: ./output/policy_models
    # Trajectory generation params
    policy_path: None # ./output/policy_models/best.pt
    num_trajectories: 20
    out_dir: ./learned_trajectories
